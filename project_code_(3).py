# -*- coding: utf-8 -*-
"""project code (3).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TNu3RBGs3g4pgHaULxtWRIcbRlgNNzN0
"""

import pandas as pd 
import numpy as np 
from sklearn import preprocessing
from sklearn import metrics
import seaborn as sns
from sklearn.svm import SVC
from matplotlib import pyplot as plt
from pandas.plotting import scatter_matrix

df = pd.read_csv('stock data.csv')

df1 = df.copy()
df1.head()

df1.tail()

df1.shape

df1.isnull().sum()

df1.info()

df1.describe()

print(len(df1))

#Graphical Representation of each coloumn 
df['Open Price'].plot(figsize=(16,6))

df['Close Price'].plot(figsize=(16,6))

df['High Price'].plot(figsize=(16,6))

df['Low Price'].plot(figsize=(16,6))

df['VWAP'].plot(figsize=(16,6))

df['No.of Shares'].plot(figsize=(16,6))

df['No. of Trades'].plot(figsize=(16,6))

df['Total Turnover (Rs.)'].plot(figsize=(16,6))

"""## Finding Correlation"""

corr = df1[df1.columns].corr()
plt.figure(figsize=(10,10))
sns.heatmap(corr,annot=True)

"""#### open,high,low,close & VWAP are having high correlation b/w them

## Outliers check
"""

import warnings
warnings.filterwarnings('ignore')

#box plots
df1.plot(kind='box', subplots=True, figsize=(20,8), sharex=False, sharey=False)
plt.show()

"""#### There are many outliers in No.of Share, No.of Trades & Total Turnover"""

# Density Plots
df1.plot(kind='density', subplots=True, figsize=(16,30), sharex=False)
plt.show()

#histogram
df1.hist(figsize=(12, 8))
plt.show()

#Scatterplot Matrix
sns.set_style('darkgrid')
sns.pairplot(df1)
plt.show()

num_columns = df1.select_dtypes(exclude='object').columns.tolist()

plt.figure(figsize=(18,40))
for i,col in enumerate(num_columns,1):
    plt.subplot(8,4,i)
    sns.kdeplot(df[col],color='g',shade=True)
    plt.subplot(8,4,i+10)
    df[col].plot.box()
plt.tight_layout() 
plt.show()
num_data = df[num_columns]
pd.DataFrame(data=[num_data.skew(),num_data.kurtosis()],index=['skewness','kurtosis'])

"""## Auto EDA"""

from pandas_profiling import ProfileReport

report = ProfileReport(df1)
report

report.to_file('report.html')

data = pd.read_html("report.html")

data

!pip install sweetviz

import sweetviz as SV

my_report = SV.analyze(df1)

my_report.show_html('report.html')

"""## Model building"""

#Linear Regression
import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score

# load the data
data = pd.read_csv('stock data.csv')

# Split dataset into training and testing sets
X = data.drop(['Date', 'Close Price','No.of Shares','No. of Trades','Total Turnover (Rs.)'], axis=1)
y = data['Close Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# create and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# make predictions on the testing set
y_pred = model.predict(X_test)

# calculate the mean squared error
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("MAE:", mae)
print("MSE:", mse)
print("R2 Score:", r2)

#random forest
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load dataset
data = pd.read_csv('stock data.csv')


# Split dataset into training and testing sets
X = data.drop(['Date', 'Close Price','No.of Shares','No. of Trades','Total Turnover (Rs.)'], axis=1)
y = data['Close Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Predict on test set
y_pred = rf.predict(X_test)

# Calculate MSE
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("MAE:", mae)
print("MSE:", mse)
print("R2 Score:", r2)

#SVM
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

# Load dataset
data = pd.read_csv('stock data.csv')

# Split dataset into training and testing sets
X = data.drop(['Date', 'Close Price','No.of Shares','No. of Trades','Total Turnover (Rs.)'], axis=1)
y = data['Close Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
svm = SVR(kernel='rbf', gamma='scale')
svm.fit(X_train, y_train)

# Predict on test set
y_pred = svm.predict(X_test)

# Calculate MSE
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("MAE:", mae)
print("MSE:", mse)
print("R2 Score:", r2)

#ARIMA
import pandas as pd
import numpy as np
import os
from subprocess import check_output
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
from pandas.plotting import lag_plot
import datetime
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error

# Load the data
df = pd.read_csv('stock data.csv', parse_dates=['Date'], index_col='Date')

def smape_kun(y_true, y_pred):
    return np.mean((np.abs(y_pred - y_true) * 200/ (np.abs(y_pred) + np.abs(y_true)))) 

df.drop(['No.of Shares','No. of Trades','Total Turnover (Rs.)'], axis=1, inplace=True)

train_data, test_data = df[0:int(len(df)*0.8)], df[int(len(df)*0.8):]
train_ar = train_data['Close Price'].values
test_ar = test_data['Close Price'].values
# https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/
history = [x for x in train_ar]
print(type(history))
predictions = list()
for t in range(len(test_ar)):
    model = ARIMA(history, order=(5,1,0))
    model_fit = model.fit()
    output = model_fit.forecast()
    yhat = output[0]
    predictions.append(yhat)
    obs = test_ar[t]
    history.append(obs)
    #print('predicted=%f, expected=%f' % (yhat, obs))
error = mean_squared_error(test_ar, predictions)
print('Testing Mean Squared Error: %.3f' % error)
error2 = smape_kun(test_ar, predictions)
print('Symmetric mean absolute percentage error: %.3f' % error2)

#LSTM
import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

# Load and preprocess the data
data = pd.read_csv('stock data.csv')
data = data['Close Price'].values.reshape(-1, 1)
scaler = MinMaxScaler(feature_range=(0, 1))
data = scaler.fit_transform(data)

# Split data into training and testing sets
train_size = int(len(data) * 0.8)
test_size = len(data) - train_size
train_data, test_data = data[0:train_size,:], data[train_size:len(data),:]

# Define function to create time series dataset
def create_dataset(dataset, look_back=1):
    X, Y = [], []
    for i in range(len(dataset)-look_back):
        X.append(dataset[i:(i+look_back), 0])
        Y.append(dataset[i+look_back, 0])
    return np.array(X), np.array(Y)

# Create time series dataset
look_back = 10
train_X, train_Y = create_dataset(train_data, look_back)
test_X, test_Y = create_dataset(test_data, look_back)

# Reshape input data to be [samples, time steps, features]
train_X = np.reshape(train_X, (train_X.shape[0], train_X.shape[1], 1))
test_X = np.reshape(test_X, (test_X.shape[0], test_X.shape[1], 1))

# Build LSTM model
lstm_model = Sequential()
lstm_model.add(LSTM(50, input_shape=(look_back, 1)))
lstm_model.add(Dense(1))
lstm_model.compile(loss='mean_squared_error', optimizer='adam')

# Train model
lstm_model.fit(train_X, train_Y, epochs=100, batch_size=32)

# Make predictions
train_predict = lstm_model.predict(train_X)
test_predict = lstm_model.predict(test_X)

# Invert predictions
train_predict = scaler.inverse_transform(train_predict)
train_Y = scaler.inverse_transform([train_Y])
test_predict = scaler.inverse_transform(test_predict)
test_Y = scaler.inverse_transform([test_Y])

# Calculate root mean squared error
train_score = mean_squared_error(train_Y[0], train_predict[:,0], squared=False)
print('Train Score: %.2f RMSE' % (train_score))
test_score = mean_squared_error(test_Y[0], test_predict[:,0], squared=False)
print('Test Score: %.2f RMSE' % (test_score))

train_score = mean_squared_error(train_Y[0], train_predict[:,0], squared=True)
print('Train Score: %.2f MSE' % (train_score))
test_score = mean_squared_error(test_Y[0], test_predict[:,0], squared=True)
print('Test Score: %.2f MSE' % (test_score))

import streamlit as st
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from keras.models import load_model

# Load the model
model = lstm_model

# Load the scaler
scaler = MinMaxScaler(feature_range=(0,1))
scaler.fit_transform(pd.read_csv('stock data.csv')['Close Price'].values.reshape(-1,1))

# Define the Streamlit app
st.set_page_config(page_title='Stock Price Prediction App', layout='wide')
st.title('Stock Price Prediction App')
st.write('Enter the past 30 days closing prices of a stock to predict the next day closing price.')

# Define the input form
input_form = st.form(key='input_form')
input_data = input_form.text_input(label='Enter the past 30 days closing prices separated by commas')
submit_button = input_form.form_submit_button(label='Predict')

# Define the prediction function
def predict(input_data):
    # Convert the input data to a numpy array
    input_data = np.array(input_data.split(','), dtype=float)
    
    # Scale the input data
    scaled_data = scaler.transform(input_data.reshape(-1,1))
    
    # Convert the data into input sequences
    n_steps = 30
    n_features = 1
    X = np.array([scaled_data[i-n_steps:i, 0] for i in range(n_steps, len(scaled_data))])
    X = np.reshape(X, (X.shape[0], X.shape[1], n_features))
    
    # Make the prediction and inverse transform the result
    y_pred = model.predict(X)
    result = scaler.inverse_transform(y_pred)[-1][0]
    
    # Return the result as a string
    return '{:.2f}'.format(result)

# Make the prediction when the submit button is clicked
if submit_button:
    prediction = predict(input_data)
    st.write('The predicted next day closing price is:', prediction)

!pip install streamlit

"""# New Section"""